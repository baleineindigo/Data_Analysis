{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "변수선택.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyNAv1W3Oykh/y26DafbyJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baleineindigo/Data_Analysis/blob/master/%EB%B3%80%EC%88%98%EC%84%A0%ED%83%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx4B60odETlM"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.datasets import load_digits\r\n",
        "from sklearn.model_selection import validation_curve\r\n",
        "import torch\r\n",
        "\r\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y57OX_I08VSF"
      },
      "source": [
        "## 특징 선택\r\n",
        "실무에서는 대규모의 데이터를 기반으로 분류예측 모형을 만들어야 하는 경우가 많다. \r\n",
        "\r\n",
        "대규모의 데이터라고 하면 표본의 갯수가 많거나 아니면 독립변수 즉, 특징데이터의 종류가 많거나 혹은 이 두가지 모두인 경우가 있다. \r\n",
        "\r\n",
        "여기에서는 특징데이터의 종류가 많은 경우에 가장 중요하다고 생각되는 특징데이터만 선택하여 특징데이터의 종류를 줄이기 위한 방법에 대해 알아본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obMXanS98gFR"
      },
      "source": [
        "%time\r\n",
        "# 데이터 셋\r\n",
        "from sklearn.datasets import fetch_rcv1\r\n",
        "rcv_train = fetch_rcv1(subset=\"train\")\r\n",
        "rcv_test = fetch_rcv1(subset=\"test\")\r\n",
        "X_train = rcv_train.data\r\n",
        "Y_train = rcv_train.target\r\n",
        "X_test = rcv_test.data\r\n",
        "Y_test = rcv_test.target\r\n",
        "\r\n",
        "# Ont-Hot-Encoding된 라벨을 정수형으로 복원\r\n",
        "classes = np.arange(rcv_train.target.shape[1])\r\n",
        "Y_train = Y_train.dot(classes)\r\n",
        "Y_test = Y_test.dot(classes)\r\n",
        "\r\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNWuGmxpTlT0"
      },
      "source": [
        "print(rcv_train.target.shape)\r\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J23W_scQ_W8"
      },
      "source": [
        "### 1. 분산에 의한 선택¶\r\n",
        "원래 예측모형에서 중요한 특징데이터란 종속데이터와의 상관관계가 크고 예측에 도움이 되는 데이터를 말한다. 하지만 상관관계 계산에 앞서 특징데이터의 값 자체가 표본에 따라 그다지 변하지 않는다면 종속데이터 예측에도 도움이 되지 않을 가능성이 높다. \r\n",
        "\r\n",
        "따라서 표본 변화에 따른 데이터 값의 변화 즉, 분산이 기준치보다 낮은 특징 데이터는 사용하지 않는 방법이 분산에 의한 선택 방법이다. 예를 들어 종속데이터와 특징데이터가 모두 0 또는 1 두가지 값만 가지는데 종속데이터는 0과 1이 균형을 이루는데 반해 특징데이터가 대부분(예를 들어 90%)의 값이 0이라면 이 특징데이터는 분류에 도움이 되지 않을 가능성이 높다.\r\n",
        "\r\n",
        "하지만 분산에 의한 선택은 반드시 상관관계와 일치한다는 보장이 없기 때문에 신중하게 사용해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-uW8noNQZ2y"
      },
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\r\n",
        "\r\n",
        "\r\n",
        "selector=VarianceThreshold(1e-5)\r\n",
        "X_train_sel=selector.fit_transform(X_train)\r\n",
        "X_test_sel=selector.transform(X_test)\r\n",
        "X_test_sel.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D53UsiiaRICh"
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "% time\r\n",
        "# 변수를 추려내지 않고 모델링\r\n",
        "model = BernoulliNB()\r\n",
        "model.fit(X_train, Y_train)\r\n",
        "print(\"train accuracy:{:5.3f}\".format(accuracy_score(Y_train, model.predict(X_train))))\r\n",
        "print(\"test accuracy :{:5.3f}\".format(accuracy_score(Y_test, model.predict(X_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jterOIH5SI-O"
      },
      "source": [
        "# 분산이 작은 변수만 선정\r\n",
        "result=model.fit(X_train_sel, Y_train)\r\n",
        "print(\"train accuracy:{:5.3f}\".format(accuracy_score(Y_train, result.predict(X_train_sel))))\r\n",
        "print(\"test accuracy :{:5.3f}\".format(accuracy_score(Y_test, result.predict(X_test_sel))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpc5YCy4Ui59"
      },
      "source": [
        "### 2.단일 변수 선택\r\n",
        "단일 변수 선택법은 각각의 독립변수를 하나만 사용한 예측모형의 성능을 이용하여 가장 분류성능 혹은 상관관계가 높은 변수만 선택하는 방법이다. 사이킷런 패키지의 feature_selection 서브패키지는 다음 성능지표를 제공한다.\r\n",
        "\r\n",
        "- chi2: 카이제곱 검정 통계값\r\n",
        "\r\n",
        "- f_classif: 분산분석(ANOVA) F검정 통계값\r\n",
        "\r\n",
        "- mutual_info_classif: 상호정보량(mutual information)\r\n",
        "\r\n",
        "하지만 단일 변수의 성능이 높은 특징만 모았을 때 전체 성능이 반드시 향상된다는 보장은 없다.\r\n",
        "\r\n",
        "feature_selection 서브패키지는 성능이 좋은 변수만 사용하는 전처리기인 SelectKBest 클래스도 제공한다. 사용법은 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTJf-7HuVrWT"
      },
      "source": [
        "from sklearn.feature_selection import chi2, SelectKBest\r\n",
        "\r\n",
        "selector1=SelectKBest(chi2,k=14330)\r\n",
        "X_train1=selector.fit_transform(X_train,Y_train)\r\n",
        "X_test1=selector.transform(X_test)\r\n",
        "\r\n",
        "model.fit(X_train1,Y_train)\r\n",
        "print(\"train accuracy:{:5.3f}\".format(accuracy_score(Y_train, model.predict(X_train1))))\r\n",
        "print(\"test accuracy :{:5.3f}\".format(accuracy_score(Y_test, model.predict(X_test1))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3fCbWfnXMWP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qot4SDbAYSrZ"
      },
      "source": [
        "### 다른 모형을 이용한 특성 중요도 계산\r\n",
        "특성 중요도(feature importance)를 계산할 수 있는 랜덤포레스트 등의 다른 모형을 사용하여 일단 특성을 선택하고 최종 분류는 다른 모형을 사용할 수도 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9uziJuSYs5K"
      },
      "source": [
        "# from sklearn.feature_selection import SelectFromModel\r\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\r\n",
        "\r\n",
        "%time\r\n",
        "n_sample = 10000\r\n",
        "idx = np.random.choice(range(len(Y_train)), n_sample)\r\n",
        "# model_sel = ExtraTreesClassifier(n_estimators=50).fit(X_train[idx, :],Y_train[idx])\r\n",
        "# np.argmax(model_sel.feature_importances_)\r\n",
        "# SelectFromModel로 중요도 산출\r\n",
        "# selector = SelectFromModel(model_sel, prefit=True, max_features=14330)\r\n",
        "X_train_sel = selector.transform(X_train)\r\n",
        "X_test_sel = selector.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOE-Rt8yY2kO"
      },
      "source": [
        "model.fit(X_train_sel,Y_train)\r\n",
        "print(\"train accuracy:{:5.3f}\".format(accuracy_score(Y_train, model.predict(X_train_sel))))\r\n",
        "print(\"test accuracy :{:5.3f}\".format(accuracy_score(Y_test, model.predict(X_test_sel))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAVwVD1gbARp"
      },
      "source": [
        "*** VarianceThreshold?***\r\n",
        "----\r\n",
        "Feature selector that removes all low-variance features.\r\n",
        "This feature selection algorithm looks only at the features (X), not the\r\n",
        "desired outputs (y), and can thus be used for unsupervised learning.\r\n",
        "\r\n",
        "Parameters\r\n",
        "---------\r\n",
        "threshold : float, optional\r\n",
        "    Features with a training-set variance lower than this threshold will be removed. The default is to keep all features with non-zero variance \r\n",
        "\\\r\n",
        "i.e. remove the features that have the same value in all samples.\r\n",
        "\r\n",
        "Attributes\r\n",
        "----------\r\n",
        "variances_ : array, shape (n_features,)\r\n",
        "    Variances of individual features.\r\n",
        "\r\n",
        "Notes\r\n",
        "-----\r\n",
        "Allows NaN in the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftx0WUaTZ5Ar"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MUshZjRacVm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
