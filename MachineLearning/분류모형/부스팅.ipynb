{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "부스팅.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP2CnIzmrpEB"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import torch\r\n",
        "\r\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nB3_D-_LPw3"
      },
      "source": [
        "## 에이다 부스트\r\n",
        "에이다부스트는 위원회에 넣을 개별 모형 km을 선별하는 방법으로는 학습 데이터 집합의 i번째 데이터에 가중치 wi를 주고 분류 모형이 틀리게 예측한 데이터의 가중치를 합한 값을 손실함수 L로 사용한다. 이 손실함수를 최소화하는 모형이 km으로 선택된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6pBaJ4gtg5h"
      },
      "source": [
        "from sklearn.datasets import make_gaussian_quantiles\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "\r\n",
        "\r\n",
        "x1,y1=make_gaussian_quantiles(\r\n",
        "    cov=2, n_samples=100,n_features=2,\r\n",
        "    n_classes=2,random_state=1\r\n",
        "    )\r\n",
        "\r\n",
        "x2,y2=make_gaussian_quantiles(\r\n",
        "    mean=(3,3),cov=1.5,\r\n",
        "    n_samples=200,n_features=2,\r\n",
        "    n_classes=2,random_state=1\r\n",
        "    )\r\n",
        "\r\n",
        "X=np.concatenate((x1,x2))\r\n",
        "Y=np.concatenate((y1, - y2 + 1))\r\n",
        "class MyAdaBoostClassifier(AdaBoostClassifier):\r\n",
        "    \r\n",
        "    def __init__(self,\r\n",
        "                 base_estimator=None,\r\n",
        "                 n_estimators=50,\r\n",
        "                 learning_rate=1.,\r\n",
        "                 algorithm='SAMME.R',\r\n",
        "                 random_state=None):\r\n",
        "\r\n",
        "        super(MyAdaBoostClassifier, self).__init__(\r\n",
        "            base_estimator=base_estimator,\r\n",
        "            n_estimators=n_estimators,\r\n",
        "            learning_rate=learning_rate,\r\n",
        "            random_state=random_state)\r\n",
        "        self.sample_weight = [None] * n_estimators\r\n",
        "        \r\n",
        "    def _boost(self, iboost, X, y, sample_weight, random_state):\r\n",
        "        sample_weight, estimator_weight, estimator_error = \\\r\n",
        "        super(MyAdaBoostClassifier, self)._boost(iboost, X, y, sample_weight, random_state)\r\n",
        "        self.sample_weight[iboost] = sample_weight.copy()\r\n",
        "        return sample_weight, estimator_weight, estimator_error\r\n",
        "    \r\n",
        "model_ada = MyAdaBoostClassifier(DecisionTreeClassifier(max_depth=1, random_state=0), n_estimators=20)\r\n",
        "model_ada.fit(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHw6-vrwtgoB"
      },
      "source": [
        "\r\n",
        "def plot_result(model, title=\"분류결과\", legend=False, s=50):\r\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\r\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\r\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02), np.arange(x2_min, x2_max, 0.02))\r\n",
        "    if isinstance(model, list):\r\n",
        "        Y = model[0].predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\r\n",
        "        for i in range(len(model) - 1):\r\n",
        "            Y += model[i + 1].predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\r\n",
        "    else:\r\n",
        "        Y = model.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\r\n",
        "    cs = plt.contourf(xx1, xx2, Y, cmap=plt.cm.Paired, alpha=0.5)\r\n",
        "    for i, n, c in zip(range(2), \"01\", \"br\"):\r\n",
        "        idx = np.where(y == i)\r\n",
        "        plt.scatter(X[idx, 0], X[idx, 1], c=c, s=s, alpha=0.5, label=\"Class %s\" % n)\r\n",
        "    plt.xlim(x1_min, x1_max)\r\n",
        "    plt.ylim(x2_min, x2_max)\r\n",
        "    plt.xlabel('x1')\r\n",
        "    plt.ylabel('x2')\r\n",
        "    plt.title(title)\r\n",
        "    plt.colorbar(cs)\r\n",
        "    if legend:\r\n",
        "        plt.legend()\r\n",
        "    plt.grid(False)\r\n",
        "\r\n",
        "plot_result(model_ada, \"AdaBoost(m=20) Classification Result\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz0TuyP3KqzY"
      },
      "source": [
        "## 에이다부스트 모형의 정규화\r\n",
        "에이다부스트 모형이 과최적화가 되는 경우에는 학습 속도(learning rate) 조정하여 정규화를 할 수 있다. 이는 필요한 멤버의 수를 강제로 증가시켜서 과최적화를 막는 역할을 한다.\r\n",
        "\r\n",
        "에이다부스트 모형이 과최적화가 되는 경우에는 학습 속도(learning rate) 조정하여 정규화를 할 수 있다. 이는 필요한 멤버의 수를 강제로 증가시켜서 과최적화를 막는 역할을 한다.\r\n",
        "\r\n",
        "Cm=Cm−1+μαmkm\r\n",
        "\r\n",
        "AdaBoostClassifier 클래스에서는 learning_rate 인수를 1보다 적게 주면 새로운 멤버의 가중치를 강제로 낮춘다.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC4qM82-Kq6i"
      },
      "source": [
        "%%time \r\n",
        "\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "mean_test_accuracy = []\r\n",
        "for n in np.arange(1, 1001, 100):\r\n",
        "    model1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=n)\r\n",
        "    mean_test_accuracy.append(cross_val_score(model1, X, Y, cv=5).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4NvGN9qIGHF"
      },
      "source": [
        "plt.plot(np.arange(1, 1000, 100), mean_test_accuracy)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IhqpLXGL5Jn"
      },
      "source": [
        "## 그레디언트 부스트\r\n",
        "그레디언트 부스트 모형에서는 손실 범함수(loss functional) L(y,Cm−1)을 최소화하는 개별 분류함수 km를 찾는다. 이론적으로 가장 최적의 함수는 범함수의 미분이다.\r\n",
        "\r\n",
        "따라서 그레디언트 부스트 모형은 분류/회귀 문제에 상관없이 개별 멤버 모형으로 회귀분석 모형을 사용한다. 가장 많이 사용되는 회귀분석 모형은 의사결정 회귀나무(decision tree regression model) 모형이다.\r\n",
        "\r\n",
        "그레디언트 부스트 모형에서는 다음과 같은 과정을 반복하여 멤버와 그 가중치를 계산한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB_d70SqPjdc"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\r\n",
        "model_grad=GradientBoostingClassifier(\r\n",
        "    n_estimators=100, max_depth=2, random_state=0\r\n",
        ")\r\n",
        "\r\n",
        "%time\r\n",
        "model_grad.fit(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfSVhN8gK_rM"
      },
      "source": [
        "plot_result(model_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI8BohCVQ_1U"
      },
      "source": [
        "plot_result(model_grad.estimators_[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvvHfaeIRBN0"
      },
      "source": [
        "## XGBoost\r\n",
        "import xgboost\r\n",
        "model_xgb = xgboost.XGBClassifier(n_estimators=100, max_depth=1, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtnyTM10TVRQ"
      },
      "source": [
        "model_xgb.fit(X,Y)\r\n",
        "plot_result(model_xgb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFcWwbrHTZOQ"
      },
      "source": [
        "## lightgbm\r\n",
        "import lightgbm\r\n",
        "\r\n",
        "model_lgbm = lightgbm.LGBMClassifier(n_estimators=100, max_depth=1, random_state=0)\r\n",
        "%time\r\n",
        "model_lgbm.fit(X, y)\r\n",
        "plot_result(model_lgbm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-fX2r4PTiA0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHfpQJ5RTrdR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}